{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NEjxDCnWdSN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c86130-2f86-47b8-9435-22806bb300d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10OWo4v3Op1UQe_rCQX-laToo9wffIPIp\n",
            "To: /content/vlsp_sentiment_test.csv\n",
            "\r  0% 0.00/159k [00:00<?, ?B/s]\r100% 159k/159k [00:00<00:00, 25.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/file/d/10OWo4v3Op1UQe_rCQX-laToo9wffIPIp/view?usp=sharing --fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sd1H5AuGdAUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090d81aa-10b1-42d1-e756-663a88c7a634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CEC49GoeBrCBY3gTGoIFiW0YB5LOZXVb\n",
            "To: /content/vlsp_sentiment_train.csv\n",
            "\r  0% 0.00/858k [00:00<?, ?B/s]\r100% 858k/858k [00:00<00:00, 116MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/file/d/1CEC49GoeBrCBY3gTGoIFiW0YB5LOZXVb/view?usp=sharing --fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l7K3HKvGdwwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8b00b6-b1d5-4fa2-ca2a-8403199c0d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1d4hk9wBL_MYr0_XACe_67LgCooQYrThl\n",
            "From (redirected): https://drive.google.com/uc?id=1d4hk9wBL_MYr0_XACe_67LgCooQYrThl&confirm=t&uuid=760261f6-a2e8-4f00-99de-612820723d23\n",
            "To: /content/vi-model-CBOW.bin\n",
            "100% 699M/699M [00:07<00:00, 95.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/file/d/1d4hk9wBL_MYr0_XACe_67LgCooQYrThl/view?usp=sharing --fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aoVjFKmOZMZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c2c7fa-e07f-45e0-a383-c2ce70996a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8TFX7wSAmg9y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "a7lMy03omg93",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4HR1jAzImg94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e123a8a-1799-4516-d6c0-a1c7248188fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ],
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jvrbwPfZmg95"
      },
      "outputs": [],
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3HlbVeHimg95"
      },
      "outputs": [],
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Lm4OCwxXmg96"
      },
      "outputs": [],
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "nW2OZgkgmg97"
      },
      "outputs": [],
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pTb0MeDRmg98"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jW-7mKtWmg9-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-BHpPSLTmg9_"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "LlV3M2dimg9_"
      },
      "outputs": [],
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4dl9VZ3Rmg-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01866805-88d1-46ec-afa4-3418896bbfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ],
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-KKSjJdJmg-A"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "njBANdn5mg-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b953cbe-5a18-4bd2-f50d-ea8b32733798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 300)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 300, 400)             3167600   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 298, 256)             307456    ['embedding[2][0]']           \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 296, 256)             512256    ['embedding[2][0]']           \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 294, 256)             717056    ['embedding[2][0]']           \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)         (None, 298, 256)             0         ['conv1d_6[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)         (None, 296, 256)             0         ['conv1d_7[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)         (None, 294, 256)             0         ['conv1d_8[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPoolin  (None, 1, 256)               0         ['reshape_6[0][0]']           \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPoolin  (None, 1, 256)               0         ['reshape_7[0][0]']           \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPoolin  (None, 1, 256)               0         ['reshape_8[0][0]']           \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 3, 256)               0         ['max_pooling1d_6[0][0]',     \n",
            " )                                                                   'max_pooling1d_7[0][0]',     \n",
            "                                                                     'max_pooling1d_8[0][0]']     \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  (None, 1024)                 2365440   ['concatenate_2[0][0]']       \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 1024)                 0         ['bidirectional_2[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 1024)                 0         ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 3)                    3075      ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7072883 (26.98 MB)\n",
            "Trainable params: 7072883 (26.98 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n",
        "from tensorflow.keras.layers import Reshape, Flatten,LSTM,GRU,Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,5,7]\n",
        "num_filters = 256\n",
        "drop = 0.2\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "\n",
        "conv_0 = Reshape((-1, num_filters))(conv_0)\n",
        "conv_1 = Reshape((-1, num_filters))(conv_1)\n",
        "conv_2 = Reshape((-1, num_filters))(conv_2)\n",
        "\n",
        "#sequence_length - filter_sizes[0] + 1\n",
        "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
        "\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1) #\n",
        "merged_tensor = Bidirectional(GRU(512,activation = 'relu'))(merged_tensor)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "\n",
        "\n",
        "#reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.02))(dropout)\n",
        "\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)#,weight_decay = 2)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='loss', min_delta=0.1, patience=4, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Combine callbacks into a list\n",
        "callbacks_list = [model_checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Jn0dBlzjmg-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd49293-5a70-4ff7-f3eb-96ae6485ae5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 11.0704 - accuracy: 0.3647\n",
            "Epoch 1: loss improved from inf to 11.07038, saving model to best_model.h5\n",
            "10/10 [==============================] - 9s 480ms/step - loss: 11.0704 - accuracy: 0.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 7.0186 - accuracy: 0.4118\n",
            "Epoch 2: loss improved from 11.07038 to 7.01863, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 7.0186 - accuracy: 0.4118\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8777 - accuracy: 0.6382\n",
            "Epoch 3: loss improved from 7.01863 to 4.87768, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 4.8777 - accuracy: 0.6382\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3230 - accuracy: 0.7210\n",
            "Epoch 4: loss improved from 4.87768 to 3.32304, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 3.3230 - accuracy: 0.7210\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.2591 - accuracy: 0.7949\n",
            "Epoch 5: loss improved from 3.32304 to 2.25914, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 2.2591 - accuracy: 0.7949\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.8880\n",
            "Epoch 6: loss improved from 2.25914 to 1.55704, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 1.5570 - accuracy: 0.8880\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.0638 - accuracy: 0.9678\n",
            "Epoch 7: loss improved from 1.55704 to 1.06384, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 513ms/step - loss: 1.0638 - accuracy: 0.9678\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7639 - accuracy: 0.9935\n",
            "Epoch 8: loss improved from 1.06384 to 0.76387, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 0.7639 - accuracy: 0.9935\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.9400\n",
            "Epoch 9: loss did not improve from 0.76387\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 0.8291 - accuracy: 0.9400\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.1039 - accuracy: 0.8173\n",
            "Epoch 10: loss did not improve from 0.76387\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 1.1039 - accuracy: 0.8173\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.9065\n",
            "Epoch 11: loss did not improve from 0.76387\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 0.9437 - accuracy: 0.9065\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8163 - accuracy: 0.9576\n",
            "Epoch 12: loss did not improve from 0.76387\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.8163 - accuracy: 0.9576\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.9869\n",
            "Epoch 13: loss improved from 0.76387 to 0.63280, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 511ms/step - loss: 0.6328 - accuracy: 0.9869\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.9961\n",
            "Epoch 14: loss improved from 0.63280 to 0.49558, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 519ms/step - loss: 0.4956 - accuracy: 0.9961\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.9988\n",
            "Epoch 15: loss improved from 0.49558 to 0.39750, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 527ms/step - loss: 0.3975 - accuracy: 0.9988\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.9994\n",
            "Epoch 16: loss improved from 0.39750 to 0.32799, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 521ms/step - loss: 0.3280 - accuracy: 0.9994\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9998\n",
            "Epoch 17: loss improved from 0.32799 to 0.27495, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 551ms/step - loss: 0.2749 - accuracy: 0.9998\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9927\n",
            "Epoch 18: loss improved from 0.27495 to 0.25388, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 511ms/step - loss: 0.2539 - accuracy: 0.9927\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2114 - accuracy: 0.6218\n",
            "Epoch 19: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 3.2114 - accuracy: 0.6218\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.2830 - accuracy: 0.7588\n",
            "Epoch 20: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 1.2830 - accuracy: 0.7588\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.3189 - accuracy: 0.8531\n",
            "Epoch 21: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 1.3189 - accuracy: 0.8531\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.1302 - accuracy: 0.9031\n",
            "Epoch 22: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 1.1302 - accuracy: 0.9031\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8984 - accuracy: 0.9527\n",
            "Epoch 23: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 0.8984 - accuracy: 0.9527\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.9880\n",
            "Epoch 24: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 478ms/step - loss: 0.7126 - accuracy: 0.9880\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.9945\n",
            "Epoch 25: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.6040 - accuracy: 0.9945\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.9984\n",
            "Epoch 26: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 479ms/step - loss: 0.5188 - accuracy: 0.9984\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.9990\n",
            "Epoch 27: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 0.4520 - accuracy: 0.9990\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.9994\n",
            "Epoch 28: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.3953 - accuracy: 0.9994\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.9996\n",
            "Epoch 29: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.3495 - accuracy: 0.9996\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9996\n",
            "Epoch 30: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.3108 - accuracy: 0.9996\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.9998\n",
            "Epoch 31: loss did not improve from 0.25388\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.2784 - accuracy: 0.9998\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 1.0000\n",
            "Epoch 32: loss improved from 0.25388 to 0.25116, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 510ms/step - loss: 0.2512 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 1.0000\n",
            "Epoch 33: loss improved from 0.25116 to 0.22837, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 513ms/step - loss: 0.2284 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 1.0000\n",
            "Epoch 34: loss improved from 0.22837 to 0.20926, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 510ms/step - loss: 0.2093 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9221 - accuracy: 0.6916\n",
            "Epoch 35: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 4.9221 - accuracy: 0.6916\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.4252 - accuracy: 0.7380\n",
            "Epoch 36: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 1.4252 - accuracy: 0.7380\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.5892 - accuracy: 0.8308\n",
            "Epoch 37: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 1.5892 - accuracy: 0.8308\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.4475 - accuracy: 0.8845\n",
            "Epoch 38: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 1.4475 - accuracy: 0.8845\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.2243 - accuracy: 0.9335\n",
            "Epoch 39: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 1.2243 - accuracy: 0.9335\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.0213 - accuracy: 0.9739\n",
            "Epoch 40: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 1.0213 - accuracy: 0.9739\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.9922\n",
            "Epoch 41: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.8699 - accuracy: 0.9922\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7621 - accuracy: 0.9967\n",
            "Epoch 42: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.7621 - accuracy: 0.9967\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.9990\n",
            "Epoch 43: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.6774 - accuracy: 0.9990\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.9996\n",
            "Epoch 44: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.6086 - accuracy: 0.9996\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.9990\n",
            "Epoch 45: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.5529 - accuracy: 0.9990\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.9986\n",
            "Epoch 46: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.5082 - accuracy: 0.9986\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.9976\n",
            "Epoch 47: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.4752 - accuracy: 0.9976\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4625 - accuracy: 0.9945\n",
            "Epoch 48: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.4625 - accuracy: 0.9945\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.9973\n",
            "Epoch 49: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.4446 - accuracy: 0.9973\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.9982\n",
            "Epoch 50: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.4224 - accuracy: 0.9982\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.9994\n",
            "Epoch 51: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.3928 - accuracy: 0.9994\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.9929\n",
            "Epoch 52: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.3809 - accuracy: 0.9929\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.8973\n",
            "Epoch 53: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.6918 - accuracy: 0.8973\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.9567\n",
            "Epoch 54: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.6034 - accuracy: 0.9567\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.9853\n",
            "Epoch 55: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.5692 - accuracy: 0.9853\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.9980\n",
            "Epoch 56: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.4939 - accuracy: 0.9980\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.9996\n",
            "Epoch 57: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 0.4303 - accuracy: 0.9996\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.9996\n",
            "Epoch 58: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.3758 - accuracy: 0.9996\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9996\n",
            "Epoch 59: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.3319 - accuracy: 0.9996\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9996\n",
            "Epoch 60: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.2973 - accuracy: 0.9996\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9998\n",
            "Epoch 61: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.2695 - accuracy: 0.9998\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9998\n",
            "Epoch 62: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.2466 - accuracy: 0.9998\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 1.0000\n",
            "Epoch 63: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 0.2274 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 1.0000\n",
            "Epoch 64: loss did not improve from 0.20926\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.2113 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 1.0000\n",
            "Epoch 65: loss improved from 0.20926 to 0.19742, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 510ms/step - loss: 0.1974 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 1.0000\n",
            "Epoch 66: loss improved from 0.19742 to 0.18530, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 507ms/step - loss: 0.1853 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 1.0000\n",
            "Epoch 67: loss improved from 0.18530 to 0.17442, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 510ms/step - loss: 0.1744 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9998\n",
            "Epoch 68: loss improved from 0.17442 to 0.16505, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 508ms/step - loss: 0.1651 - accuracy: 0.9998\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9994\n",
            "Epoch 69: loss improved from 0.16505 to 0.15908, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 517ms/step - loss: 0.1591 - accuracy: 0.9994\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9992\n",
            "Epoch 70: loss improved from 0.15908 to 0.15768, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 521ms/step - loss: 0.1577 - accuracy: 0.9992\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9822\n",
            "Epoch 71: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 0.2142 - accuracy: 0.9822\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9724\n",
            "Epoch 72: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.3103 - accuracy: 0.9724\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.9843\n",
            "Epoch 73: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.3606 - accuracy: 0.9843\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.9924\n",
            "Epoch 74: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.3505 - accuracy: 0.9924\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.9988\n",
            "Epoch 75: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 0.3164 - accuracy: 0.9988\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9998\n",
            "Epoch 76: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.2745 - accuracy: 0.9998\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 1.0000\n",
            "Epoch 77: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.2346 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 1.0000\n",
            "Epoch 78: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.2027 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 1.0000\n",
            "Epoch 79: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 0.1778 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 1.0000\n",
            "Epoch 80: loss did not improve from 0.15768\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 0.1585 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 1.0000\n",
            "Epoch 81: loss improved from 0.15768 to 0.14338, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 510ms/step - loss: 0.1434 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 1.0000\n",
            "Epoch 82: loss improved from 0.14338 to 0.13110, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 522ms/step - loss: 0.1311 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 1.0000\n",
            "Epoch 83: loss improved from 0.13110 to 0.12103, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 507ms/step - loss: 0.1210 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 1.0000\n",
            "Epoch 84: loss improved from 0.12103 to 0.11228, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 507ms/step - loss: 0.1123 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 1.0000\n",
            "Epoch 85: loss improved from 0.11228 to 0.10482, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 511ms/step - loss: 0.1048 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 1.0000\n",
            "Epoch 86: loss improved from 0.10482 to 0.09823, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 508ms/step - loss: 0.0982 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 1.0000\n",
            "Epoch 87: loss improved from 0.09823 to 0.09247, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 524ms/step - loss: 0.0925 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 1.0000\n",
            "Epoch 88: loss improved from 0.09247 to 0.08723, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 508ms/step - loss: 0.0872 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 1.0000\n",
            "Epoch 89: loss improved from 0.08723 to 0.08256, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 508ms/step - loss: 0.0826 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 1.0000\n",
            "Epoch 90: loss improved from 0.08256 to 0.07830, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 514ms/step - loss: 0.0783 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 1.0000\n",
            "Epoch 91: loss improved from 0.07830 to 0.07449, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 509ms/step - loss: 0.0745 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 1.0000\n",
            "Epoch 92: loss improved from 0.07449 to 0.07115, saving model to best_model.h5\n",
            "10/10 [==============================] - 5s 525ms/step - loss: 0.0712 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2926 - accuracy: 0.7186\n",
            "Epoch 93: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 5.2926 - accuracy: 0.7186\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.7185 - accuracy: 0.6041\n",
            "Epoch 94: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 1.7185 - accuracy: 0.6041\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.0441 - accuracy: 0.7786\n",
            "Epoch 95: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 2.0441 - accuracy: 0.7786\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8250 - accuracy: 0.8739\n",
            "Epoch 96: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 1.8250 - accuracy: 0.8739\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.5764 - accuracy: 0.9286\n",
            "Epoch 97: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 1.5764 - accuracy: 0.9286\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.3306 - accuracy: 0.9731\n",
            "Epoch 98: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 1.3306 - accuracy: 0.9731\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.1480 - accuracy: 0.9947\n",
            "Epoch 99: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 1.1480 - accuracy: 0.9947\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.0153 - accuracy: 0.9986\n",
            "Epoch 100: loss did not improve from 0.07115\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 1.0153 - accuracy: 0.9986\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c47f2624bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model.fit(data, labels,  epochs=100, batch_size=512, callbacks=callbacks_list, shuffle=True) #validation_split=0.2,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8XoN2UOamg-D"
      },
      "outputs": [],
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "PwiYb3Ohmg-E"
      },
      "outputs": [],
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "E08tBw9img-E"
      },
      "outputs": [],
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OwgI9Xywmg-E"
      },
      "outputs": [],
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "p02GxCh6mg-F"
      },
      "outputs": [],
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "jAqUMGInmg-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2989520d-23b1-4500-f9b6-0dffd7e677ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ],
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LKclttiOmg-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb157df6-1d37-45a7-c48d-7742712ffeb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 1s 15ms/step - loss: 2.3404 - accuracy: 0.6790\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "r31_uxxgmg-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fb29a4-5cac-44c8-cc92-350ca47a303a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 234.04\n",
            "accuracy: 67.90%\n"
          ]
        }
      ],
      "source": [
        "print(\"%s: %.2f\" % (model.metrics_names[0], score[0]*100))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "v8O3z4IFmg-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cbce63-18b2-49a9-d17a-c24574e6bc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 1s 12ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predictions.argmax(axis = 1 )"
      ],
      "metadata": {
        "id": "rb3m0ewuLe-K"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test_class = labels_test.argmax(axis = 1 )"
      ],
      "metadata": {
        "id": "fnr0jWgVLbCV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "mtrx = confusion_matrix(labels_test_class, predictions)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=mtrx,display_labels=[-1,0,1])\n",
        "disp.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "yeNUZRElL3xC",
        "outputId": "c9f8d8cc-113c-466d-eeb4-fe38e47ad3cd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7c481ddafd60>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGwCAYAAADfWt0SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+sklEQVR4nO3deVhUZfsH8O8ZYIZtBgSEAUVccCO3XELKXJIE7eeS9pa9VGhmZWKlaS65oSVvWmqaaatkr7anJZmFuyZaomhuJIiBsrggICjbnPP7g9exCS3GGRiG5/u5rnNdzjnPeeYm0rnnvp9zjqQoigIiIiISisrWARAREVHdYwJAREQkICYAREREAmICQEREJCAmAERERAJiAkBERCQgJgBEREQCcrR1ANYmyzKys7Oh1WohSZKtwyEiIjMpioIrV64gICAAKlXtfE8tLS1FeXm5VeZSq9Vwdna2ylx1qcElANnZ2QgMDLR1GEREZKGsrCw0bdrU6vOWlpaiRZA7cs8brDKfXq9HRkaG3SUBDS4B0Gq1AID3dofAxd3BxtFQbftg3FBbh0B1yCkjz9YhUB2olMux48LHxn/Pra28vBy55w34I7k5dFrLKgxFV2QEdTuD8vJyJgC2dr3s7+LuAFctE4CGztHRvv7CkWUcVWpbh0B1qLbbuO5aCe5ay95Dhv22mhtcAkBERFQTBkWGwcKn4RgU2TrB2AATACIiEpIMBTIsywAsPd+WeBkgERGRgFgBICIiIcmQYWkB3/IZbIcJABERCcmgKDAolpXwLT3fltgCICIiEhArAEREJCTRFwEyASAiIiHJUGAQOAFgC4CIiEhArAAQEZGQ2AIgIiISEK8CICIiIuGwAkBEREKS/7dZOoe9YgJARERCMljhKgBLz7clJgBERCQkgwIrPA3QOrHYAtcAEBERCYgVACIiEpLoawBYASAiIiHJkGCwcJMhmfWecXFx6NGjB7RaLXx9fTFs2DCkpqaajOnbty8kSTLZnn32WZMxmZmZeOCBB+Dq6gpfX19MmTIFlZWVZsXCCgAREVEd2blzJ8aPH48ePXqgsrISM2bMwIABA3D8+HG4ubkZx40dOxbz5s0zvnZ1dTX+2WAw4IEHHoBer8fevXuRk5ODJ554Ak5OTliwYEGNY2ECQEREQpKVqs3SOcyxefNmk9fx8fHw9fVFcnIyevfubdzv6uoKvV5/0zl++uknHD9+HFu2bIGfnx+6dOmC+fPnY+rUqZg7dy7UanWNYmELgIiIhGRp+f/6BgBFRUUmW1lZWY1iKCwsBAB4eXmZ7F+7di18fHzQoUMHTJ8+HVevXjUeS0pKQseOHeHn52fcFxERgaKiIhw7dqzGPz8rAERERBYKDAw0eT1nzhzMnTv3b8+RZRkvvvgi7rnnHnTo0MG4/9///jeCgoIQEBCAI0eOYOrUqUhNTcU333wDAMjNzTX58AdgfJ2bm1vjmJkAEBGRkP78Dd6SOQAgKysLOp3OuF+j0fzjuePHj8fRo0exZ88ek/1PP/208c8dO3aEv78/+vfvj/T0dLRq1cqieP+MLQAiIhKSrEhW2QBAp9OZbP+UAMTExCAhIQHbt29H06ZN/3ZsaGgoACAtLQ0AoNfrkZeXZzLm+utbrRu4GSYAREREdURRFMTExGD9+vXYtm0bWrRo8Y/npKSkAAD8/f0BAGFhYfjtt99w/vx545jExETodDqEhITUOBa2AIiISEjWbAHU1Pjx47Fu3Tp8++230Gq1xp69h4cHXFxckJ6ejnXr1mHQoEHw9vbGkSNHMHHiRPTu3RudOnUCAAwYMAAhISF4/PHHsXDhQuTm5mLmzJkYP358jVoP1zEBICIiIRmggsHCQrjBzPErV64EUHWznz9bvXo1Ro0aBbVajS1btmDp0qUoKSlBYGAgRowYgZkzZxrHOjg4ICEhAePGjUNYWBjc3NwQHR1tct+AmmACQEREQlL+1MO3ZA7zxv/9jQMCAwOxc+fOf5wnKCgImzZtMuu9/4prAIiIiATECgAREQnJFmsA6hMmAEREJCSDooJBsXANgIW3ErYltgCIiIgExAoAEREJSYYE2cLvwTLstwTABICIiIQk+hoAtgCIiIgExAoAEREJyTqLANkCICIisitVawAsK+Fber4tsQVAREQkIFYAiIhISLIVngXAqwCIiIjsDNcAEBERCUiGSuj7AHANABERkYBYASAiIiEZFAkGCx8HbOn5tsQEgIiIhGSwwiJAA1sAREREZE9YASAiIiHJigqyhVcByLwKgIiIyL6wBUBERETCYQWAiIiEJMPyVfyydUKxCSYAREQkJOvcCMh+C+n2GzkRERHdNlYAiIhISNZ5FoD9fo9mAkBEREKSIUGGpWsAeCdAIiIiu8IKANVLB1c1wumf3FBwWg0HjQx911L0nHIJjVpWGMfsnNkYZ/e6ouS8A5xcFei7Xqsa06rCZK6TX2txeLUnCjOc4OQuo9XAYvSee7GufyQyw5oVX0HvW1Jt/3eb2+LtD3vCycmAZ574FX3vOQMnJwMOpARg+Qc9UVDoYoNoyZr+NSoDo54/hQ3rmuH9N9r95aiC2OUH0f2eS5g/qQv27fC1SYzUMNSrBOCbb77BqlWrkJycjPz8fBw6dAhdunSxdVg2kf2LMzpEFcK3UxnkSmD/m95IGB2AkT9kwsm16sYTjTuUofWQK3APqERZoQN+XeaFhNEBiNr+B1QOVfMc/sgThz/yRM+XL8Kvcykqr6lQdK5e/drpJiZM/z+oVDduMNI88DJen52IXUnNAQDPjvoFoV3P4dXFfVByVY3xY/ZjzuTtmDhrkI0iJmtoHVKIyBFZOP27+02PD4vKhGLHD5+pb6xzIyD7rQDUq8hLSkrQq1cvvP7667YOxeb+76MctBtxBV6ty+HTvhz3vZ6H4mwnXDiqMY4JGVmEgLtKoWtaicZ3lCF04iUU5zjhytmqD/iyQhV+WeKF+xbmoc2QYngEVcK7XTla9L9qqx+LaqiwyBmXC1yMW2i3sziXq8WR435wdS1H5H1pePfj7kg56o9Tp73x5op7cEe7C2jX+oKtQ6fb5OxSiSmv/Ybl8+9AcZFTteMt2xThwcfO4K3YO2wQXcMkK5JVNntVr74KPv744wCAM2fO2DaQeqi8uOorvcbz5redqLgq4eTXOmibVsDdvxIAkPWzKxQZKMlzxKcRzVBRooK+6zXcPf2ScQzVf46OBvS/9zS+TggBIKFNy0twcpRx8EiAcUxWtgfyLrghpM15nDzV2HbB0m0bN+0Eft3jg5RfvPHIU6dNjmmcDZiy4Des/E97XL6kucUMROapVwnA7SgrK0NZWZnxdVFRkQ2jqR2KDPz8qg/03a7Bu025ybGja3VIWuiDyqsqeLYsx+D4c3BQVx0rynKEokg4uKoR7pl5EWp3A35Z6o2NowLw8MZM4ziq3+7ukQV3t3L8tCMYANDI8xrKK1QouWr6C7xc6IxGnqW2CJEs1HtADoLbXcGLj4fe9PjYl1Jx4rAn9u1kz9+aZCu0AHgjIBuKi4uDh4eHcQsMDLR1SFa3a25j5J9S4/4ludWOtR5SjH99m4Wha8/Co3kFfnpBj8qyqpKUIkuQKyT0mnkBze69Cv2dZbh/cS4Kzzjh3H7Xuv4x6DZF3ncKvx5qgvzL/J01RD5+pXh6SioWzeyIinKHasdDe59Hpx75eO+NtjaIrmG7/jRASzd7ZbPI165dC3d3d+O2e/fu25pn+vTpKCwsNG5ZWVlWjtS2dsf64I/trhjyyTm4+xuqHddoZXg2r0DAXaWIWJ6DgtNqZPzkBgBwa1xV5m8UfKNq4OItw7mRAcXZdl/8EYKvTzHu7JSDH7a2Nu67XOACtZMMN1fTalAjj1JcLnCu6xDJQsHti9DIuxzL1u7Dd78k4rtfEtGp+2UMGZmJ735JxJ09L8G/6VV8sXO78TgAzFiUgrj3frVx9GTPbPYpMGTIEISG3ih3NWnS5Lbm0Wg00GgaXk9MUYA983yQkeiOIf89B11gDXr2StVmKK+qAOi7VZWDCzLUcPe/BgAoLVCh9LIDtAEVt5qF6pGIfmkoKHTG/oNNjft+P+2NikoV7uyYgz37gwAATQMK4de4BMd/Z4nY3hz+xQvP/SvMZN+Lc4/h7Bk3fBXfHEUFavzwdVOT4+98mYT332yLX3ZxvYclDJBgsPBGPpaeb0s2SwC0Wi20Wq2t3r7e2z23MU5tdMfAlTlQu8m4eqGqNKjWynB0VlCU6Yi0TVoE9roKZy8DSnIdcfDdRnBwVtCsb9Uqf88WFWgeXow9r/qg76sX4OQuY/8b3vBsWY6Antds+eNRDUiSggH90pC4sxVk+Uax7upVNTZvC8Yz0b/iSrEaV6+p8dyT+3EstTEXANqha1cd8Ue66b+FpdccUFToZNx/s4V/F3JdkJfNtpAlrFHCt+cWQL2qA+fn5yMzMxPZ2dkAgNTUVACAXq+HXq+3ZWh17tg6DwDAt4+ZZv79/pOHdiOuwEGjIOeAM47Ee6CsyAEu3pUI6FGKBz8/C1fvG62C/gvz8POCxvh+rD8kFRDQ4xr+76McOFS/yojqma4ds+HXuAQ/bguudmxV/F1Q5F8xa/IOqB1lHDhcdSMgIqKakhRFUf55WN2Ij4/H6NGjq+2fM2cO5s6dW6M5ioqK4OHhgU8OdYSrtvqCGmpY3hn1kK1DoDrklJ5j6xCoDlTK5diS9z4KCwuh0+msPv/1z4nZ+8Ph7G7Zt6HS4grMC91Sa7HWpnpVARg1ahRGjRpl6zCIiEgAbAEQEREJSPSHAdlv5ERERHTbWAEgIiIhKZAgW3gZn8LLAImIiOwLWwBEREQkHFYAiIhISNZ4nC8fB0xERGRnDFZ4GqCl59uS/UZOREREt40VACIiEhJbAERERAKSoYJsYSHc0vNtyX4jJyIiotvGCgAREQnJoEgwWFjCt/R8W2ICQEREQuIaACIiIgEpVngaoMI7ARIREZE9YQWAiIiEZIAEg4UP87H0fFtiAkBEREKSFct7+LJipWBsgC0AIiIiAbECQEREQpKtsAjQ0vNtiQkAEREJSYYE2cIevqXn25L9pi5ERER021gBICIiIfFOgERERAISfQ2A/UZOREREt40VACIiEpIMKzwLwI4XATIBICIiISlWuApAYQJARERkX0R/GiDXABAREQmIFQAiIhISrwIgIiIS0PUWgKWbOeLi4tCjRw9otVr4+vpi2LBhSE1NNRlTWlqK8ePHw9vbG+7u7hgxYgTy8vJMxmRmZuKBBx6Aq6srfH19MWXKFFRWVpoVCxMAIiKiOrJz506MHz8e+/btQ2JiIioqKjBgwACUlJQYx0ycOBEbN27El19+iZ07dyI7OxvDhw83HjcYDHjggQdQXl6OvXv34uOPP0Z8fDxmz55tVixsARARkZCs+SyAoqIik/0ajQYajaba+M2bN5u8jo+Ph6+vL5KTk9G7d28UFhbiww8/xLp163DfffcBAFavXo327dtj37596NmzJ3766SccP34cW7ZsgZ+fH7p06YL58+dj6tSpmDt3LtRqdY1iZwWAiIiEZM0WQGBgIDw8PIxbXFxcjWIoLCwEAHh5eQEAkpOTUVFRgfDwcOOYdu3aoVmzZkhKSgIAJCUloWPHjvDz8zOOiYiIQFFREY4dO1bjn58VACIiIgtlZWVBp9MZX9/s2/9fybKMF198Effccw86dOgAAMjNzYVarYanp6fJWD8/P+Tm5hrH/PnD//rx68dqigkAEREJyZr3AdDpdCYJQE2MHz8eR48exZ49eyyK4XaxBUBEREKyxVUA18XExCAhIQHbt29H06ZNjfv1ej3Ky8tRUFBgMj4vLw96vd445q9XBVx/fX1MTTABICIiqiOKoiAmJgbr16/Htm3b0KJFC5Pj3bp1g5OTE7Zu3Wrcl5qaiszMTISFhQEAwsLC8Ntvv+H8+fPGMYmJidDpdAgJCalxLGwBEBGRkGxxK+Dx48dj3bp1+Pbbb6HVao09ew8PD7i4uMDDwwNjxozBpEmT4OXlBZ1OhwkTJiAsLAw9e/YEAAwYMAAhISF4/PHHsXDhQuTm5mLmzJkYP358jdYeXMcEgIiIhKTA8qf5KWaOX7lyJQCgb9++JvtXr16NUaNGAQCWLFkClUqFESNGoKysDBEREXjnnXeMYx0cHJCQkIBx48YhLCwMbm5uiI6Oxrx588yKhQkAEREJyRYVAEX555TB2dkZK1aswIoVK245JigoCJs2bTLrvf+KawCIiIgExAoAEREJSfTHATMBICIiIYmeALAFQEREJCBWAIiISEiiVwCYABARkZAURYJi4Qe4pefbElsAREREAmIFgIiIhCRDsvhGQJaeb0tMAIiISEiirwFgC4CIiEhArAAQEZGQRF8EyASAiIiEJHoLgAkAEREJSfQKANcAEBERCajBVgDee+FBODo62zoMqmUhb/1m6xCoDv0+to2tQ6A6YDCUAXm1/z6KFVoA9lwBaLAJABER0d9RACiK5XPYK7YAiIiIBMQKABERCUmGBIl3AiQiIhILrwIgIiIi4bACQEREQpIVCRJvBERERCQWRbHCVQB2fBkAWwBEREQCYgWAiIiEJPoiQCYAREQkJCYAREREAhJ9ESDXABAREQmIFQAiIhKS6FcBMAEgIiIhVSUAlq4BsFIwNsAWABERkYBYASAiIiHxKgAiIiIBKf/bLJ3DXrEFQEREJCBWAIiISEhsARAREYlI8B4AEwAiIhKTFSoAsOMKANcAEBERCYgVACIiEhLvBEhERCQg0RcBsgVAREQkIFYAiIhITIpk+SI+O64AMAEgIiIhib4GgC0AIiIiAbECQEREYuKNgIiIiMQj+lUANUoAvvvuuxpPOGTIkNsOhoiIiOpGjRKAYcOG1WgySZJgMBgsiYeIiKju2HEJ31I1SgBkWa7tOIiIiOqU6C0Ai64CKC0ttVYcREREdUux0manzE4ADAYD5s+fjyZNmsDd3R2nT58GAMyaNQsffvih1QMkIiIi6zM7AXjttdcQHx+PhQsXQq1WG/d36NABH3zwgVWDIyIiqj2SlTb7ZHYCsGbNGrz33nuIioqCg4ODcX/nzp1x8uRJqwZHRERUa9gCMM+5c+cQHBxcbb8sy6ioqLBKUERERFS7zE4AQkJCsHv37mr7v/rqK9x5551WCYqIiKjWCV4BMPtOgLNnz0Z0dDTOnTsHWZbxzTffIDU1FWvWrEFCQkJtxEhERGR9gj8N0OwKwNChQ7Fx40Zs2bIFbm5umD17Nk6cOIGNGzfi/vvvr40YiYiIyMpu61kA9957LxITE60dCxERUZ0R/XHAt/0woAMHDuDEiRMAqtYFdOvWzWpBERER1To+DdA8Z8+exaOPPoqff/4Znp6eAICCggLcfffd+Oyzz9C0aVNrx0hERERWZvYagKeeegoVFRU4ceIE8vPzkZ+fjxMnTkCWZTz11FO1ESMREZH1XV8EaOlmp8yuAOzcuRN79+5F27Ztjfvatm2L5cuX495777VqcERERLVFUqo2S+ewV2YnAIGBgTe94Y/BYEBAQIBVgiIiIqp1gq8BMLsFsGjRIkyYMAEHDhww7jtw4ABeeOEFvPHGG1YNjoiIiGpHjSoAjRo1giTd6HOUlJQgNDQUjo5Vp1dWVsLR0RFPPvkkhg0bViuBEhERWZXgNwKqUQKwdOnSWg6DiIiojgneAqhRAhAdHV3bcRAREVEduu0bAQFAaWkpysvLTfbpdDqLAiIiIqoTglcAzF4EWFJSgpiYGPj6+sLNzQ2NGjUy2YiIiOyCDZ4GuGvXLgwePBgBAQGQJAkbNmwwOT5q1ChIkmSyRUZGmozJz89HVFQUdDodPD09MWbMGBQXF5sXCG4jAXj55Zexbds2rFy5EhqNBh988AFiY2MREBCANWvWmB0AERGRKEpKStC5c2esWLHilmMiIyORk5Nj3D799FOT41FRUTh27BgSExORkJCAXbt24emnnzY7FrNbABs3bsSaNWvQt29fjB49Gvfeey+Cg4MRFBSEtWvXIioqyuwgiIiI6pwVrwIoKioy2a3RaKDRaKoNHzhwIAYOHPi3U2o0Guj1+pseO3HiBDZv3oxff/0V3bt3BwAsX74cgwYNwhtvvGHW/XjMrgDk5+ejZcuWAKr6/fn5+QCAXr16YdeuXeZOR0REZBPX7wRo6QZU3STPw8PDuMXFxd12XDt27ICvry/atm2LcePG4dKlS8ZjSUlJ8PT0NH74A0B4eDhUKhX2799v1vuYXQFo2bIlMjIy0KxZM7Rr1w5ffPEF7rrrLmzcuNH4cCCqHf9d+iX0jav3eb5NbIfl8WF485Uf0Dkk1+TYxq1t8dZHd9dViHSbLq02oHi7grIzClQawKWThMYTHKBufuPbScE3Moo2yyhLVSCXAMHbHeGgvXG8IlvBpQ8MuHpAQeUlwNEH0A1SwftJFSQn+71WWQTe3lcxZnQKunfLhkZjQHaOOxYv6YlTad4AgMf+fQR9emeiceMSVFSqkJbmhfg1nZGa6mPjyOm6rKwsk0XwN/v2XxORkZEYPnw4WrRogfT0dMyYMQMDBw5EUlISHBwckJubC19fX5NzHB0d4eXlhdzc3FvMenNmJwCjR4/G4cOH0adPH0ybNg2DBw/G22+/jYqKCixevNjc6W5qxYoVWLRoEXJzc9G5c2csX74cd911l1XmtmfjZw2GSiUbX7doWoCFM37Erv3Njfu+39YG8V/daXxdVm7RhR5UR64eVOD5LxWcQyQoBuDiCgOyYirR4ktHqFyqPrzlUgVud0twu1vCxbflanOUn1GgKIDfDAc4NZVQnq4g9zUD5GuA74sOdf0jUQ25u5dj8aJEHD7ih5lz+qKw0BlNAq6guFhtHHP2nA7vrOqOnFx3aNSVeHBYKhbM344nnxqMwiJnG0Zv56x4FYBOp7PKVXAjR440/rljx47o1KkTWrVqhR07dqB///4Wz/9nZn86TJw40fjn8PBwnDx5EsnJyQgODkanTp0sDujzzz/HpEmTsGrVKoSGhmLp0qWIiIhAampqtaxHNIVXTP+ijxz8G87lanH4xI1eUWmZIy4XutZ1aGShwOWmfxX1cx2Qfn8lSk8ocO1alQB4/bvqQ/zqgeof/gDgdrcKbnff6Oqpm0rw+kNBwdcyE4B67F8PHceFC65YvLSncV9enrvJmB07m5u8fu/9roiMSEeLFgVIOXzzXjE1DC1btoSPjw/S0tLQv39/6PV6nD9/3mRMZWUl8vPzb7lu4FYs/noYFBSEoKAgS6cxWrx4McaOHYvRo0cDAFatWoXvv/8eH330EaZNm2a197F3jg4GhPdKx1eb7gBwo7zb/550hPdKR36BC/YdCsR/13dhFcAOyf/r9DjoLCvdG4oBlYVzUO3qGXoWyQf98cr03ejY4TwuXnJFwvetsfnH4JuOd3Q0YODANBQXO+F0hmfdBtvASLDC0wCtEsmtnT17FpcuXYK/vz8AICwsDAUFBUhOTka3bt0AANu2bYMsywgNDTVr7hp9MixbtqzGEz7//PNmBfBn5eXlSE5OxvTp0437VCoVwsPDkZSUdNNzysrKUFZWZnz915WYDdU93TPh7lqOn3a1Nu7btrcl8i6641KBC1oEXsbYRw+gqX8hYpdat2xEtUuRFZx/0wCXzhI0wbf/z0t5loKCz2U05rf/es1fX4z/G3QK36xvh88+vwNt2uRj3DPJqKxUYcvWlsZxd/U4h+lTf4ZGU4n8fBfMmHkfilj+tzvFxcVIS0szvs7IyEBKSgq8vLzg5eWF2NhYjBgxAnq9Hunp6Xj55ZcRHByMiIgIAED79u0RGRmJsWPHYtWqVaioqEBMTAxGjhxp9hN5a5QALFmypEaTSZJkUQJw8eJFGAwG+Pn5mez38/PDyZMnb3pOXFwcYmNjb/s97dXAvr/jl8NNcangRrn/++1tjX/OyPJCfoEL3njlR/j7FiHnPO/QaC/yXpdRlq6g2Qe3X7mpOK/g7IRKaMMleD5o9sU+VIckCTiV5oX4NV0AAOmnvdA8qAAPDDxlkgAcPuKH5yYMhIeuDAMj0zBj2h68MCkChYVMAm6bDR4GdODAAfTr18/4etKkSQCqbrm/cuVKHDlyBB9//DEKCgoQEBCAAQMGYP78+SaLCteuXYuYmBj0798fKpUKI0aMMOuL+nU1+hcmIyPD7InryvTp043/AYGqCkBgYKANI6p9vj7FuLNDDmKX9vvbcSfTGwMAmvhdYQJgJ/JeN6Bkj4zA9xzh5Hd7/zBVXlCQ9WwlXDpJ8HuF3/7ru/zLzsjM9DDZl5nlgXvuzjLZV1bmiJwcLXJytDiZ6oMP3/sOkQPS8fmXd9RluA2LDW4F3LdvXyjKrU/68ccf/3EOLy8vrFu3zrw3vol61Rz28fGBg4MD8vLyTPbn5eXdcnHDrW620JBF9j6FgkJn7Dv094lOq6CqezRcKnCpi7DIAoqi4PxCGcU7ZAS+6wh1k9v78K84X/Xh79xOgn6OAyQV+//13fHjjdG0iWnrskmTIpy/4Pa350kqwMnJUJuhUQNXr2qDarUa3bp1w9atW437ZFnG1q1bERYWZsPI6g9JUhDR5xQSdwdDlm/8+vx9ixA1LAWtm1+En88VhHXNxNRnd+PwCT9kZHnZMGKqifOvyyj6QYb/qw5QuQKVFxVUXlQgl974plB5UUFpqoLys1Wvy9KqXhsKq8ZUnFeQ9UwlnPQSGr/oAMPlG/NQ/bV+Qzu0a3cRjzx8DP7+V9C3zxkMikzDxoSq9T0aTSVGPZGCdm0vwrdxCYKD8zHxhX3w8b6K3Xua2Th6O2eDZwHUJ/WqAgBU9UOio6PRvXt33HXXXVi6dClKSkqMVwWIrmuHbPj5lOCHna1N9ldWqtC1QzZGRB6Hs6YS5/NdsfvXIKzd0NlGkZI5Cr6qurQv6xnTb3T6OQ7wGFz1Lb7gaxmX3r9xCWDWWIPJmKv7FVRkARVZCk4PqjSZp+0Bp9oMnyzw+ylvzHu1N0aPSkHUo78hN88dq97rhu07WgAAZFlCYGARwvvvhs6jDFeKNPj9lBcmv3w//sj0tG3wdu7Pd/KzZA57Ve8SgEceeQQXLlzA7NmzkZubiy5dumDz5s3VFgaKKvm3JgiPqp4MXch3x0uvDrJBRGQNNfmA9nnGAT7P3Lqn7zFYBY/B9aqoRzX0y69N8MuvTW56rKLCAfNf613HEZEI6l0CAAAxMTGIiYmxdRhERNSQ2WARYH1yW18Xdu/ejcceewxhYWE4d+4cAOCTTz7Bnj17rBocERFRrRF8DYDZCcDXX3+NiIgIuLi44NChQ8ab8BQWFmLBggVWD5CIiIisz+wE4NVXX8WqVavw/vvvw8npRt/ynnvuwcGDB60aHBERUW2x5uOA7ZHZawBSU1PRu3f1BSkeHh4oKCiwRkxERES1zwZ3AqxPzK4A6PV6k/sYX7dnzx60bNnyJmcQERHVQ1wDYJ6xY8fihRdewP79+yFJErKzs7F27VpMnjwZ48aNq40YiYiIyMrMbgFMmzYNsiyjf//+uHr1Knr37g2NRoPJkydjwoQJtREjERGR1fFGQGaSJAmvvPIKpkyZgrS0NBQXFyMkJATu7u61ER8REVHtEPw+ALd9IyC1Wo2QkBBrxkJERER1xOwEoF+/fpCkW6963LZtm0UBERER1QlrXMYnUgWgS5cuJq8rKiqQkpKCo0ePIjo62lpxERER1S62AMyzZMmSm+6fO3cuiouLLQ6IiIiIap/VHh322GOP4aOPPrLWdERERLVL8PsAWO1pgElJSXB2drbWdERERLWKlwGaafjw4SavFUVBTk4ODhw4gFmzZlktMCIiIqo9ZicAHh4eJq9VKhXatm2LefPmYcCAAVYLjIiIiGqPWQmAwWDA6NGj0bFjRzRq1Ki2YiIiIqp9gl8FYNYiQAcHBwwYMIBP/SMiIrsn+uOAzb4KoEOHDjh9+nRtxEJERER1xOwE4NVXX8XkyZORkJCAnJwcFBUVmWxERER2Q9BLAAEz1gDMmzcPL730EgYNGgQAGDJkiMktgRVFgSRJMBgM1o+SiIjI2gRfA1DjBCA2NhbPPvsstm/fXpvxEBERUR2ocQKgKFVpTp8+fWotGCIiorrCGwGZ4e+eAkhERGRX2AKouTZt2vxjEpCfn29RQERERFT7zEoAYmNjq90JkIiIyB6xBWCGkSNHwtfXt7ZiISIiqjuCtwBqfB8A9v+JiIgaDrOvAiAiImoQBK8A1DgBkGW5NuMgIiKqU1wDQEREJCLBKwBmPwuAiIiI7B8rAEREJCbBKwBMAIiISEiirwFgC4CIiEhArAAQEZGY2AIgIiISD1sAREREJBxWAIiISExsARAREQlI8ASALQAiIiIBsQJARERCkv63WTqHvWICQEREYhK8BcAEgIiIhMTLAImIiEg4rAAQEZGY2AIgIiISlB1/gFuKLQAiIiIBsQJARERCEn0RIBMAIiISk+BrANgCICIiEhArAEREJCS2AIiIiETEFgARERGJpsFWABx2HYaD5GTrMKiWpT3RxtYhUB3avGWtrUOgOlB0RUajOvirzRYAERGRiARvATABICIiMQmeAHANABERkYBYASAiIiFxDQAREZGI2AIgIiIi0bACQEREQpIUBZJi2Vd4S8+3JVYAiIhITIqVNjPs2rULgwcPRkBAACRJwoYNG0xDUhTMnj0b/v7+cHFxQXh4OE6dOmUyJj8/H1FRUdDpdPD09MSYMWNQXFxsXiBgAkBERFRnSkpK0LlzZ6xYseKmxxcuXIhly5Zh1apV2L9/P9zc3BAREYHS0lLjmKioKBw7dgyJiYlISEjArl278PTTT5sdC1sAREQkJGteBVBUVGSyX6PRQKPRVBs/cOBADBw48KZzKYqCpUuXYubMmRg6dCgAYM2aNfDz88OGDRswcuRInDhxAps3b8avv/6K7t27AwCWL1+OQYMG4Y033kBAQECNY2cFgIiIxGTFFkBgYCA8PDyMW1xcnNnhZGRkIDc3F+Hh4cZ9Hh4eCA0NRVJSEgAgKSkJnp6exg9/AAgPD4dKpcL+/fvNej9WAIiIiCyUlZUFnU5nfH2zb///JDc3FwDg5+dnst/Pz894LDc3F76+vibHHR0d4eXlZRxTU0wAiIhISNZsAeh0OpMEwB6wBUBERGKywVUAf0ev1wMA8vLyTPbn5eUZj+n1epw/f97keGVlJfLz841jaooJABERCel6BcDSzVpatGgBvV6PrVu3GvcVFRVh//79CAsLAwCEhYWhoKAAycnJxjHbtm2DLMsIDQ016/3YAiAiIqojxcXFSEtLM77OyMhASkoKvLy80KxZM7z44ot49dVX0bp1a7Ro0QKzZs1CQEAAhg0bBgBo3749IiMjMXbsWKxatQoVFRWIiYnByJEjzboCAGACQEREorLBswAOHDiAfv36GV9PmjQJABAdHY34+Hi8/PLLKCkpwdNPP42CggL06tULmzdvhrOzs/GctWvXIiYmBv3794dKpcKIESOwbNkys0NnAkBERMKq66f59e3bF8rf3D5YkiTMmzcP8+bNu+UYLy8vrFu3zuJYuAaAiIhIQKwAEBGRmBSlarN0DjvFBICIiIRkzfsA2CO2AIiIiATECgAREYnJBlcB1CdMAIiISEiSXLVZOoe9YguAiIhIQKwAEBGRmNgCICIiEo/oVwEwASAiIjEJfh8ArgEgIiISECsAREQkJLYAiIiIRCT4IkC2AIiIiATECgAREQmJLQAiIiIR8SoAIiIiEg0rAEREJCS2AIiIiETEqwCIiIhINKwAEBGRkNgCICIiEpGsVG2WzmGnmAAQEZGYuAaAiIiIRMMKABERCUmCFdYAWCUS22ACQEREYuKdAImIiEg0rAAQEZGQeBkgERGRiHgVABEREYmGFQAiIhKSpCiQLFzEZ+n5tsQEgIiIxCT/b7N0DjvFFgAREZGAWAEgIiIhsQVAREQkIsGvAmACQEREYuKdAImIiEg0rAAQEZGQeCdAshuPxOThnkGFCAwuQ3mpCscPuOLD1/xxNt3ZOKZR4wo8NSsHXXtfgau7jKx0DT57yxd7NnnaLnAyW9QTxxD1xHGTfVmZWjzzZCR8/UoQv3bTTc9bMK8n9uwKrIsQ6TZ9ttwXP2/yRFaaBmpnGSHdr2LMK9kIDC4zGXf8gCviX/fHyYOucHAAWt5xDQvWpUPjUvWJ88RdIcg7qzY558np2Xhkwvk6+1nsnuAtACYAdqRTWAk2xvvg9xRXODgqGDUtBws+PY2xfdqi7JoDAGDKsky46wyYO6oFCvMd0O/BAsx49w9MGKhG+lFXG/8EZI4zGTq88nIf42uDoerBoxcvuCLqX4NNxkY+cBojHk7FgV/86zRGMt+RJHcMHnURbbpchaESiP+PP2Y82grv7zwJZ9eqi8qPH3DFK1GtMDImD8+9eg4ODgpOH3eB9Jem7RNTcjAw6pLxtau7HV+UTnWu3iUAu3btwqJFi5CcnIycnBysX78ew4YNs3VY9cIrUS1NXr/5YjN8cfQYWne6hqP73QEAId2vYvm0JkhNqfqw//QtPwwfewGtO11jAmBnDAYJly87V9svy9X3393rHHbvbIrS0nr3V5r+YsG60yavX1qaiUc6dsSpIy7o2LMEAPDu3CYYNuaCybf5v1YIAMDFXYaXb2XtBtyASXLVZukc9qreLQIsKSlB586dsWLFCluHUu+56QwAgCsFDsZ9xw+4os+QAmg9KyFJCvoMvQy1s4Ije91tFSbdpiZNivHJZxvx4SebMGX6fjT2vXrTccGtL6NVcAF++qFFHUdI1lBSVPX3V+tZ9fe54KIjTh50g6d3JV4c3BqPdLoDk4cH4+h+t2rnfvG2Lx66owOeu78NvnynMQzMBcxzvQVg6Wan6t3XhYEDB2LgwIE1Hl9WVoayshuZcVFRUW2EVe9IkoJnY8/h6C+u+CPVxbj/tWeaY8aqM/jq+DFUVgBl11SIHdMc2Wc0NoyWzJV6wguLF/XA2SwtvLxL8e/Hj2PRku0Y99QAXLvmZDJ2wMAMZP6hxYnjPjaKlm6XLAOr5jTBHT2K0bxdKQAg54+qvv4ni/UYOysbre64hi1fNcK0R1rh3W0n0aRlOQBg6JgLCO54DVrPShw/4IbVcf7IP++EZ+Zm2+znIftS7xIAc8XFxSE2NtbWYdS5mAXnENSuFC8NCzbZH/1yDtx1MqY+3BJF+Y4IiyzEK6vO4KUHg3HmpMstZqP65sCvN3r5ZzKqEoL4dd/j3j5n8dPmG9/01WoD+t6XiU//294WYZKF3p7RFH+cdMGbG04Z98n/KykPeuwSIkbmAwCCO15Dyh4tfvzMG0/OyAEAjHjmgvGcliGlcHJS8NbUQIyengO1xn6/ldYpwW8EVO9aAOaaPn06CgsLjVtWVpatQ6p14187i9D7i/DyQ61wMefGKmD/oDIMffISFk8KRMoeLU4fd8HaxXqcOuKKIaMu/c2MVN+VlKhx7qwWAU2KTfb36n0WGk0ltiY2t01gdNventEE+xN1WPhVGhoHVBj3e/tV1fGD2pSajA8MLsX5c6bVnz9r2/UqDJUS8rLUtxxDpq7fCtjSzV7ZfQVAo9FAoxGlvK1g/GvncHdkIaY8FIy8LNOfW+NS9dVB/suiFIMBkFT2+z8pAc7OlfD3L8a2S0Em+wcMzMD+pAAUFYryd8D+KQqw4pUm2LvZA4u+SoO+WbnJcb/Acnjry3E23fR3eu60Bt3vu3LLeU8fc4FKpcDThwsBqGbsPgEQScyCc+j34GXMHd0C14pVaNS46ltDyRUHlJeqkJXmjHOn1Xhh4Vm8Py8ARZcdcHdkIbr2LsbsJ7hAzJ6Mefow9u8LwPk8V3h7X8Nj0ccgyxJ2bG9mHOMfUIwOHS9gziv32jBSMtfbM5pi+/pGmLv6NFzcZeSfr/pn2E1rgMZFgSQBD427gE/e0KNlyDW0vOMatnzphax0Z8x8/wyAqsW+Jw+5ofPdVff7OJHshlVzAnDfiMvGxYRUA7wPANmLwf8r47/xTbrJ/jdeDETiF14wVEqY+XhLjJmRg9iPM+DiJiM7Q403XgjEr9t0tgiZbpNP42uYOmMfdLpyFBZqcOyoDyZO6G/yTX9AZAYuXnTBwQN+NoyUzJXwcdVizSkjWpvsf2lJJgY8UtXzHz72AipKJaya0wRXChzQMqQUcZ+mI6B5VbXASa1g57ee+O+belSUS9AHlmP40xcw/OkLIDMoACy9jM9+P/8hKUr9Sl+Ki4uRlpYGALjzzjuxePFi9OvXD15eXmjWrNk/nF11FYCHhwf6YigcpVv3y6hhcAhpY+sQqA5t2vKFrUOgOlB0RUajNqdRWFgInc76X16uf07cd+c0ODpUv9eGOSoNpdh26D+1FmttqncVgAMHDqBfv37G15MmTQIAREdHIz4+3kZRERERNSz1LgHo27cv6llRgoiIGiIFVlgDYJVIbKLeJQBERER1QvBFgHZ/HwAiIiIyHysAREQkJhmAZIU57BQTACIiEpI17uRnz3cCZAuAiIhIQKwAEBGRmARfBMgEgIiIxCR4AsAWABERkYBYASAiIjEJXgFgAkBERGLiZYBERETi4WWAREREJBxWAIiISExcA0BERCQgWQEkCz/AZftNANgCICIiEhArAEREJCbBWwCsABARkaCUG0nA7W4wLwGYO3cuJEky2dq1a2c8XlpaivHjx8Pb2xvu7u4YMWIE8vLyrPxzV2ECQEREVIfuuOMO5OTkGLc9e/YYj02cOBEbN27El19+iZ07dyI7OxvDhw+vlTjYAiAiIjFZsQVQVFRksluj0UCj0dz0FEdHR+j1+mr7CwsL8eGHH2LdunW47777AACrV69G+/btsW/fPvTs2dOyWP+CFQAiIhKTrFhnAxAYGAgPDw/jFhcXd8u3PXXqFAICAtCyZUtERUUhMzMTAJCcnIyKigqEh4cbx7Zr1w7NmjVDUlKS1X98VgCIiIgslJWVBZ1OZ3x9q2//oaGhiI+PR9u2bZGTk4PY2Fjce++9OHr0KHJzc6FWq+Hp6Wlyjp+fH3Jzc60eMxMAIiISkyJXbZbOAUCn05kkALcycOBA4587deqE0NBQBAUF4YsvvoCLi4tlsZiJLQAiIhKTpVcAWGENgaenJ9q0aYO0tDTo9XqUl5ejoKDAZExeXt5N1wxYigkAERGJyYprAG5XcXEx0tPT4e/vj27dusHJyQlbt241Hk9NTUVmZibCwsIs/WmrYQuAiIiojkyePBmDBw9GUFAQsrOzMWfOHDg4OODRRx+Fh4cHxowZg0mTJsHLyws6nQ4TJkxAWFiY1a8AAJgAEBGRqGxwJ8CzZ8/i0UcfxaVLl9C4cWP06tUL+/btQ+PGjQEAS5YsgUqlwogRI1BWVoaIiAi88847lsV4C0wAiIhITAqskACYN/yzzz772+POzs5YsWIFVqxYYUFQNcM1AERERAJiBYCIiMQk+MOAmAAQEZGYZBmAhfcBkC0834bYAiAiIhIQKwBERCQmtgCIiIgEJHgCwBYAERGRgFgBICIiMckKzL6Q/6Zz2CcmAEREJCRFkaFY+DRAS8+3JSYAREQkJsXyh/lwDQARERHZFVYAiIhITIoV1gDYcQWACQAREYlJlgHJwh6+Ha8BYAuAiIhIQKwAEBGRmNgCICIiEo8iy1AsbAHY82WAbAEQEREJiBUAIiISE1sAREREApIVQBI3AWALgIiISECsABARkZgUBYCl9wGw3woAEwAiIhKSIitQLGwBKEwAiIiI7Iwiw/IKAC8DJCIiIjvCCgAREQmJLQAiIiIRCd4CaHAJwPVsrBIVFt/fgeo/xVBm6xCoDhVdsd9/bKnmioqrfs+1/e3aGp8TlaiwTjA20OASgCtXrgAA9mCTjSOhOnHS1gFQXWrUxtYRUF26cuUKPDw8rD6vWq2GXq/HnlzrfE7o9Xqo1WqrzFWXJMWeGxg3IcsysrOzodVqIUmSrcOpM0VFRQgMDERWVhZ0Op2tw6FaxN+1OET9XSuKgitXriAgIAAqVe2sVS8tLUV5eblV5lKr1XB2drbKXHWpwVUAVCoVmjZtauswbEan0wn1D4XI+LsWh4i/69r45v9nzs7OdvmhbU28DJCIiEhATACIiIgExASggdBoNJgzZw40Go2tQ6Faxt+1OPi7ptrU4BYBEhER0T9jBYCIiEhATACIiIgExASAiIhIQEwAiIiIBMQEoAH45ptvMGDAAHh7e0OSJKSkpNg6JKolK1asQPPmzeHs7IzQ0FD88ssvtg6JasGuXbswePBgBAQEQJIkbNiwwdYhUQPEBKABKCkpQa9evfD666/bOhSqRZ9//jkmTZqEOXPm4ODBg+jcuTMiIiJw/vx5W4dGVlZSUoLOnTtjxYoVtg6FGjBeBtiAnDlzBi1atMChQ4fQpUsXW4dDVhYaGooePXrg7bffBlD13IvAwEBMmDAB06ZNs3F0VFskScL69esxbNgwW4dCDQwrAER2oLy8HMnJyQgPDzfuU6lUCA8PR1JSkg0jIyJ7xQSAyA5cvHgRBoMBfn5+Jvv9/PyQm5tro6iIyJ4xAbAza9euhbu7u3HbvXu3rUMiIiI71OAeB9zQDRkyBKGhocbXTZo0sWE0VFd8fHzg4OCAvLw8k/15eXnQ6/U2ioqI7BkrAHZGq9UiODjYuLm4uNg6JKoDarUa3bp1w9atW437ZFnG1q1bERYWZsPIiMhesQLQAOTn5yMzMxPZ2dkAgNTUVACAXq/nt8MGZNKkSYiOjkb37t1x1113YenSpSgpKcHo0aNtHRpZWXFxMdLS0oyvMzIykJKSAi8vLzRr1syGkVFDwssAG4D4+PibfgjMmTMHc+fOrfuAqNa8/fbbWLRoEXJzc9GlSxcsW7bMpCVEDcOOHTvQr1+/avujo6MRHx9f9wFRg8QEgIiISEBcA0BERCQgJgBEREQCYgJAREQkICYAREREAmICQEREJCAmAERERAJiAkBERCQgJgBEREQCYgJAVAtGjRqFYcOGGV/37dsXL774Yp3HsWPHDkiShIKCgluOkSQJGzZsqPGcc+fORZcuXSyK68yZM5AkCSkpKRbNQ0S3jwkACWPUqFGQJAmSJEGtViM4OBjz5s1DZWVlrb/3N998g/nz59dobE0+tImILMWHAZFQIiMjsXr1apSVlWHTpk0YP348nJycMH369Gpjy8vLoVarrfK+Xl5eVpmHiMhaWAEgoWg0Guj1egQFBWHcuHEIDw/Hd999B+BG2f61115DQEAA2rZtCwDIysrCww8/DE9PT3h5eWHo0KE4c+aMcU6DwYBJkybB09MT3t7eePnll/HXR2z8tQVQVlaGqVOnIjAwEBqNBsHBwfjwww9x5swZ40NgGjVqBEmSMGrUKABVj/+Ni4tDixYt4OLigs6dO+Orr74yeZ9NmzahTZs2cHFxQb9+/UzirKmpU6eiTZs2cHV1RcuWLTFr1ixUVFRUG/fuu+8iMDAQrq6uePjhh1FYWGhy/IMPPkD79u3h7OyMdu3a4Z133jE7FiKqPUwASGguLi4oLy83vt66dStSU1ORmJiIhIQEVFRUICIiAlqtFrt378bPP/8Md3d3REZGGs978803ER8fj48++gh79uxBfn4+1q9f/7fv+8QTT+DTTz/FsmXLcOLECbz77rtwd3dHYGAgvv76awBVj3XOycnBW2+9BQCIi4vDmjVrsGrVKhw7dgwTJ07EY489hp07dwKoSlSGDx+OwYMHIyUlBU899RSmTZtm9n8TrVaL+Ph4HD9+HG+99Rbef/99LFmyxGRMWloavvjiC2zcuBGbN2/GoUOH8NxzzxmPr127FrNnz8Zrr72GEydOYMGCBZg1axY+/vhjs+MholqiEAkiOjpaGTp0qKIoiiLLspKYmKhoNBpl8uTJxuN+fn5KWVmZ8ZxPPvlEadu2rSLLsnFfWVmZ4uLiovz444+KoiiKv7+/snDhQuPxiooKpWnTpsb3UhRF6dOnj/LCCy8oiqIoqampCgAlMTHxpnFu375dAaBcvnzZuK+0tFRxdXVV9u7dazJ2zJgxyqOPPqooiqJMnz5dCQkJMTk+derUanP9FQBl/fr1tzy+aNEipVu3bsbXc+bMURwcHJSzZ88a9/3www+KSqVScnJyFEVRlFatWinr1q0zmWf+/PlKWFiYoiiKkpGRoQBQDh06dMv3JaLaxTUAJJSEhAS4u7ujoqICsizj3//+N+bOnWs83rFjR5O+/+HDh5GWlgatVmsyT2lpKdLT01FYWIicnByEhoYajzk6OqJ79+7V2gDXpaSkwMHBAX369Klx3Glpabh69Sruv/9+k/3l5eW48847AQAnTpwwiQMAwsLCavwe133++edYtmwZ0tPTUVxcjMrKSuh0OpMxzZo1Q5MmTUzeR5ZlpKamQqvVIj09HWPGjMHYsWONYyorK+Hh4WF2PERUO5gAkFD69euHlStXQq1WIyAgAI6Opn8F3NzcTF4XFxejW7duWLt2bbW5GjdufFsxuLi4mH1OcXExAOD77783+eAFqtY1WEtSUhKioqIQGxuLiIgIeHh44LPPPsObb75pdqzvv/9+tYTEwcHBarESkWWYAJBQ3NzcEBwcXOPxXbt2xeeffw5fX99q34Kv8/f3x/79+9G7d28AVd90k5OT0bVr15uO79ixI2RZxs6dOxEeHl7t+PUKhMFgMO4LCQmBRqNBZmbmLSsH7du3Ny5ovG7fvn3//EP+yd69exEUFIRXXnnFuO+PP/6oNi4zMxPZ2dkICAgwvo9KpULbtm3h5+eHgIAAnD59GlFRUWa9PxHVHS4CJPobUVFR8PHxwdChQ7F7925kZGRgx44deP7553H27FkAwAsvvID//Oc/2LBhA06ePInnnnvub6/hb968OaKjo/Hkk09iw4YNxjm/+OILAEBQUBAkSUJCQgIuXLiA4uJiaLVaTJ48GRMnTsTHH3+M9PR0HDx4EMuXLzcurHv22Wdx6tQpTJkyBampqVi3bh3i4+PN+nlbt26NzMxMfPbZZ0hPT8eyZctuuqDR2dkZ0dHROHz4MHbv3o3nn38eDz/8MPR6PQAgNjYWcXFxWLZsGX7//Xf89ttvWL16NRYvXmxWPERUe5gAEP0NV1dX7Nq1C82aNcPw4cPRvn17jBkzBqWlpcaKwEsvvYTHH38c0dHRCAsLg1arxYMPPvi3865cuRIPPfQQnnvuObRr1w5jx45FSUkJAKBJkyaIjY3FtGnT4Ofnh5iYGADA/PnzMWvWLMTFxaF9+/aIjIzE999/jxYtWgCo6st//fXX2LBhAzp37oxVq1ZhwYIFZv28Q4YMwcSJExETE4MuXbpg7969mDVrVrVxwcHBGD58OAYNGoQBAwagU6dOJpf5PfXUU/jggw+wevVqdOzYEX369EF8fLwxViKyPUm51UolIiIiarBYASAiIhIQEwAiIiIBMQEgIiISEBMAIiIiATEBICIiEhATACIiIgExASAiIhIQEwAiIiIBMQEgIiISEBMAIiIiATEBICIiEtD/A7Zu/uTr6vAfAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiy5JskDw8Qv"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}